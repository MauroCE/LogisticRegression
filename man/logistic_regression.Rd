% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/logistic_regression.R
\name{logistic_regression}
\alias{logistic_regression}
\title{Logistic Regression S3 Object}
\usage{
logistic_regression(
  X,
  y,
  cost = "MLE",
  method = "BFGS",
  sigmab = 1,
  niter = 100,
  alpha = 0.1,
  gamma = 0.001
)
}
\arguments{
\item{X}{Matrix of training examples of dimensions (number of obs, number of features + 1). The
first column must be a column of 1s to fit the intercept.}

\item{y}{Column vector of 0-1 training labels of dimension (number of obs, 1).}

\item{cost}{String indicating which cost function to optimize. Options are "MLE" or "MAP". If "MAP" is
chosen an isotropic gaussian centered at zero and with `sigmab^2 * diag(ncol(X))` as its variance-covariance
matrix is placed as a prior on the coefficients. This corresponds to Ridge regularization on **all** the
coefficients, including the intercept.}

\item{method}{String indicating the optimization method used to optimize `cost`. If method is `BFGS` then
the function `optim()` is used. Otherwise, class methods are implemented for `grad_ascent()` (performing
gradient ascent) and `newton_method()`. In case they are implemented both for the `MLE` case and the `MAP`
case.}

\item{sigmab}{Standard deviation of the univariate gaussian distribution placed on each coordinate of the
vector of coefficients. It's the inverse of the regularization parameter. Should not be zero.}

\item{niter}{Number of iterations that the optimization algorithm should perform. This is passed only to
`grad_ascent()` and `newton_method()`, but not to the `optim()` function.}

\item{alpha}{Learning rate for `newton_method()`. Used to dump or enhance learning to avoid missing or
not reaching the optimal solution. Could be merged with `gamma` but defaults are different.}

\item{gamma}{Learning rate for `grad_ascent()`, used to dump or enhance learning to avoid missing or
or not reaching the optimal solution.}
}
\value{

}
\description{
Logistic Regression S3 Object
}
